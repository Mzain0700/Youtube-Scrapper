{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9ecc96-7957-49a5-a666-7f22531295a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f80f9f-6233-4f7f-a272-a82da3bb29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"Your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b41e3-55ef-4534-a52b-a4ba792d27af",
   "metadata": {},
   "source": [
    "# Get Channel videos Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204a6b75-eefc-4290-a3b5-91aa5c5bec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_videos=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8650611f-2d54-42ca-8321-5e236316a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_name = \"youtube channel name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a4c560c-4ece-4d4e-8e8f-ace8a1e3e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_videos(channel_name, api_key, max_videos=None):\n",
    "    # Initialize the YouTube API client\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "    # Search for the channel by name to get the channel ID\n",
    "    search_response = youtube.search().list(\n",
    "        q=channel_name,\n",
    "        type='channel',\n",
    "        part='id,snippet',\n",
    "        maxResults=1\n",
    "    ).execute()\n",
    "    \n",
    "    if not search_response['items']:\n",
    "        print(\"Channel not found.\")\n",
    "        return\n",
    "    \n",
    "    channel_id = search_response['items'][0]['id']['channelId']\n",
    "    channel_title = search_response['items'][0]['snippet']['title']\n",
    "    \n",
    "    # Fetch all videos from the channel\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    while True:\n",
    "        # Fetch videos in chunks (max 50 at a time)\n",
    "        max_results = 50\n",
    "        if max_videos is not None:\n",
    "            remaining_videos = max_videos - len(videos)\n",
    "            max_results = min(50, remaining_videos)  # Fetch only the remaining needed videos\n",
    "        \n",
    "        search_response = youtube.search().list(\n",
    "            channelId=channel_id,\n",
    "            part='id,snippet',\n",
    "            order='date',\n",
    "            maxResults=max_results,\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "        \n",
    "        for item in search_response['items']:\n",
    "            if item['id']['kind'] == 'youtube#video':\n",
    "                videos.append(item['id']['videoId'])\n",
    "        \n",
    "        next_page_token = search_response.get('nextPageToken')\n",
    "        if not next_page_token or (max_videos is not None and len(videos) >= max_videos):\n",
    "            break\n",
    "    \n",
    "    # If max_videos is set, slice the list to the maximum number of videos\n",
    "    if max_videos is not None:\n",
    "        videos = videos[:max_videos]\n",
    "    \n",
    "    # Fetch details and stats for each video\n",
    "    video_details = []\n",
    "    \n",
    "    for i in range(0, len(videos), 50):  # Process in chunks of 50\n",
    "        video_ids = videos[i:i + 50]\n",
    "        video_response = youtube.videos().list(\n",
    "            id=','.join(video_ids),\n",
    "            part='id,snippet,statistics'\n",
    "        ).execute()\n",
    "        \n",
    "        for video in video_response['items']:\n",
    "            details = {\n",
    "                'channel_id': channel_id,\n",
    "                'channel_name': channel_name,\n",
    "                'video_id': video['id'],\n",
    "                'title': video['snippet']['title'],\n",
    "                'description': video['snippet']['description'],\n",
    "                'published_at': video['snippet']['publishedAt'],\n",
    "                'view_count': video['statistics'].get('viewCount'),\n",
    "                'like_count': video['statistics'].get('likeCount'),\n",
    "                'dislike_count': video['statistics'].get('dislikeCount'),\n",
    "                'comment_count': video['statistics'].get('commentCount'),\n",
    "            }\n",
    "            video_details.append(details)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(video_details)\n",
    "    \n",
    "    # Generate a safe file name from the channel title\n",
    "    safe_channel_name = ''.join(e for e in channel_title if e.isalnum() or e in \"._- \")\n",
    "    \n",
    "    # Save DataFrame to CSV\n",
    "    csv_file_name = f\"{safe_channel_name}_videos.csv\"\n",
    "    df.to_csv(csv_file_name, index=False)\n",
    "    \n",
    "    print(f\"Video details saved to {csv_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ee3616-8fc4-4950-91d6-c0e096f70e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video details saved to Aik News Digital_videos.csv\n"
     ]
    }
   ],
   "source": [
    "get_channel_videos(channel_name, api_key,max_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4efeb6-4d7e-42a9-9c5c-2656e3a53f21",
   "metadata": {},
   "source": [
    "# Get Videos Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d616c697-1171-4cf3-9615-9337b17f77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = 'yourcsv.csv' #your csv having scrapped video ids and details above scrapped  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af41f82d-4653-4b5a-bc32-fbd95e0375b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_comments(video_id, api_key):\n",
    "    # Initialize the YouTube API client\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "    # List to store comments and replies\n",
    "    comments_data = []\n",
    "    \n",
    "    # Fetch comment threads (top-level comments)\n",
    "    next_page_token = None\n",
    "    \n",
    "    while True:\n",
    "        # Fetch comment threads (top-level comments)\n",
    "        comment_threads_response = youtube.commentThreads().list(\n",
    "            part='snippet,replies',\n",
    "            videoId=video_id,\n",
    "            maxResults=100,  # Maximum number of comments per page (API limit is 100)\n",
    "            pageToken=next_page_token,\n",
    "            textFormat='plainText'\n",
    "        ).execute()\n",
    "        \n",
    "        for item in comment_threads_response['items']:\n",
    "            top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment = {\n",
    "                'comment_id': item['snippet']['topLevelComment']['id'],\n",
    "                'author': top_comment['authorDisplayName'],\n",
    "                'text': top_comment['textDisplay'],\n",
    "                'like_count': top_comment['likeCount'],\n",
    "                'published_at': top_comment['publishedAt'],\n",
    "                'reply_count': item['snippet']['totalReplyCount']\n",
    "            }\n",
    "            comments_data.append(comment)\n",
    "            \n",
    "            # Check if the comment has replies\n",
    "            if item['snippet']['totalReplyCount'] > 0:\n",
    "                for reply in item.get('replies', {}).get('comments', []):\n",
    "                    reply_snippet = reply['snippet']\n",
    "                    reply_comment = {\n",
    "                        'comment_id': reply['id'],\n",
    "                        'author': reply_snippet['authorDisplayName'],\n",
    "                        'text': reply_snippet['textDisplay'],\n",
    "                        'like_count': reply_snippet['likeCount'],\n",
    "                        'published_at': reply_snippet['publishedAt'],\n",
    "                        'reply_to': item['snippet']['topLevelComment']['id']  # Indicates which comment this is a reply to\n",
    "                    }\n",
    "                    comments_data.append(reply_comment)\n",
    "        \n",
    "        # Check if there is a next page of comments\n",
    "        next_page_token = comment_threads_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    \n",
    "    return comments_data\n",
    "\n",
    "def scrape_comments_from_csv(input_csv, api_key):\n",
    "    # Open the input CSV file\n",
    "    with open(input_csv, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        # Assuming CSV has 'video_id', 'video_title', and 'channel_name' columns\n",
    "        for row in reader:\n",
    "            video_id = row['video_id']\n",
    "            video_title = row['title']\n",
    "            channel_name = row['channel_name']\n",
    "            \n",
    "            # Scrape comments for the video\n",
    "            comments = get_video_comments(video_id, api_key)\n",
    "            \n",
    "            # Prepare the output CSV file name\n",
    "            output_csv = f\"{channel_name}_comments.csv\"\n",
    "            \n",
    "            # Write comments to the output CSV file\n",
    "            with open(output_csv, 'a', newline='', encoding='utf-8') as output_file:\n",
    "                writer = csv.writer(output_file)\n",
    "                \n",
    "                # Write the header only if the file is empty\n",
    "                if output_file.tell() == 0:\n",
    "                    writer.writerow(['video_id', 'video_title', 'comment_id', 'author', 'text', 'like_count', 'published_at', 'reply_to'])\n",
    "                \n",
    "                for comment in comments:\n",
    "                    writer.writerow([\n",
    "                        video_id,\n",
    "                        video_title,\n",
    "                        comment.get('comment_id'),\n",
    "                        comment.get('author'),\n",
    "                        comment.get('text'),\n",
    "                        comment.get('like_count'),\n",
    "                        comment.get('published_at'),\n",
    "                        comment.get('reply_to', '')  # Reply_to is optional\n",
    "                    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809028cb-a883-464b-b250-36589b479817",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_comments_from_csv(input_csv, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d7e84-adcc-44b3-a2e7-6aa0fac1401c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
